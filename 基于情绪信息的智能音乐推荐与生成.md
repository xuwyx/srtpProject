# 基于情绪信息的智能音乐推荐与生成

## 一、项目成员简介

- 徐维亚：
  - 身份证：320401199711032222
  - 专业：计算机科学与技术
- 秦子谦：
  - 身份证：
  - 专业：计算机科学与技术
- 钱佳一：
  - 身份证：
  - 专业：计算机科学与技术

## 二、项目简介

本项目基于机器学习，通过大数据训练结合乐理知识，实现了对音乐按照情绪信息的分类，并能够根据情绪信息随机生成符合该情绪的音乐。

项目的底层核心技术为机器学习。

- 在音乐分类方面，本项目首先根据需求，对音乐格式的文件（mp3、wav等）进行了音乐特征的提取，选取合适的、能够表述音乐情绪的特征值（如响度、MFCC值等）。之后构建了基于LSTM的神经网络，并在拥有情绪标签的数据集上进行训练，得到分类模型。该训练好的神经网络接收的输入为固定长度音乐提取出的特征信息序列，输出为分类得到的表示情绪的Valence-Arousal值。
- 在音乐生成方面，本项目收集midi文件数据集，将midi文件转换为更加具有乐理知识的乐谱信息，采用abc notation进行乐曲的表示。之后通过构建char-rnn神经网络，进行音乐生成方面的训练。训练好的神经网络接收的输入为可以表示音乐Valence-Arousal值的相关信息（包括曲调、速度等），输出为一段完整的音乐文件，通过abc notation格式表示。之后再将其转换为midi格式则可以进行播放。

项目最终成果展示在https://github.com/xwy69/srtpProject上。

## 三、项目成果简介

#### 1. 软件名称

乐随心动

#### 2. 软件功能简介
软件分为音乐分类和生成两个部分。界面显示如下

![ainwindo](/Users/Jocey/Desktop/srtpProject/mainwindow.png)

- 音乐分类部分

  点击recognize按钮，将会跳出在本地选择歌曲的界面，选择好歌曲（MP3

  文件）后，程序将把歌曲转换成wav格式文件再用模型进行预测，输出结果示意图：

  ![howdem](/Users/Jocey/Desktop/srtpProject/showdemo.png)

  V表示歌曲的气氛指数，V越高则歌曲曲调越欢乐，A表示歌曲的激昂程度，A越高表示歌曲曲风越激昂。上图显示的是一首非常欢愉但曲风偏舒缓的歌曲。

  ​

  音乐生成部分

## 四、项目代码

- 音乐生成神经网络模型char-rnn代码

```python
import tensorflow as tf
from tensorflow.contrib import rnn
from tensorflow.contrib import legacy_seq2seq

import numpy as np

class Model():
    def __init__(self, args, training=True):
        self.args = args
        if not training:
            args.batch_size = 1
            args.seq_length = 1

        if args.model == 'rnn':
            cell_fn = rnn.BasicRNNCell
        elif args.model == 'gru':
            cell_fn = rnn.GRUCell
        elif args.model == 'lstm':
            cell_fn = rnn.BasicLSTMCell
        elif args.model == 'nas':
            cell_fn = rnn.NASCell
        else:
            raise Exception("model type not supported: {}".format(args.model))

        cells = []
        for _ in range(args.num_layers):
            cell = cell_fn(args.rnn_size)
            if training and (args.output_keep_prob < 1.0 or args.input_keep_prob < 1.0):
                cell = rnn.DropoutWrapper(cell,
                                          input_keep_prob=args.input_keep_prob,
                                          output_keep_prob=args.output_keep_prob)
            cells.append(cell)

        self.cell = cell = rnn.MultiRNNCell(cells, state_is_tuple=True)

        self.input_data = tf.placeholder(
            tf.int32, [args.batch_size, args.seq_length])
        self.targets = tf.placeholder(
            tf.int32, [args.batch_size, args.seq_length])
        self.initial_state = cell.zero_state(args.batch_size, tf.float32)

        with tf.variable_scope('rnnlm'):
            softmax_w = tf.get_variable("softmax_w",
                                        [args.rnn_size, args.vocab_size])
            softmax_b = tf.get_variable("softmax_b", [args.vocab_size])

        embedding = tf.get_variable("embedding", [args.vocab_size, args.rnn_size])
        inputs = tf.nn.embedding_lookup(embedding, self.input_data)

        # dropout beta testing: double check which one should affect next line
        if training and args.output_keep_prob:
            inputs = tf.nn.dropout(inputs, args.output_keep_prob)

        inputs = tf.split(inputs, args.seq_length, 1)
        inputs = [tf.squeeze(input_, [1]) for input_ in inputs]

        def loop(prev, _):
            prev = tf.matmul(prev, softmax_w) + softmax_b
            prev_symbol = tf.stop_gradient(tf.argmax(prev, 1))
            return tf.nn.embedding_lookup(embedding, prev_symbol)

        outputs, last_state = legacy_seq2seq.rnn_decoder(inputs, self.initial_state, cell, loop_function=loop if not training else None, scope='rnnlm')
        output = tf.reshape(tf.concat(outputs, 1), [-1, args.rnn_size])


        self.logits = tf.matmul(output, softmax_w) + softmax_b
        self.probs = tf.nn.softmax(self.logits)
        loss = legacy_seq2seq.sequence_loss_by_example(
                [self.logits],
                [tf.reshape(self.targets, [-1])],
                [tf.ones([args.batch_size * args.seq_length])])
        with tf.name_scope('cost'):
            self.cost = tf.reduce_sum(loss) / args.batch_size / args.seq_length
        self.final_state = last_state
        self.lr = tf.Variable(0.0, trainable=False)
        tvars = tf.trainable_variables()
        grads, _ = tf.clip_by_global_norm(tf.gradients(self.cost, tvars),
                args.grad_clip)
        with tf.name_scope('optimizer'):
            optimizer = tf.train.AdamOptimizer(self.lr)
        self.train_op = optimizer.apply_gradients(zip(grads, tvars))

        # instrument tensorboard
        tf.summary.histogram('logits', self.logits)
        tf.summary.histogram('loss', loss)
        tf.summary.scalar('train_loss', self.cost)

    def sample(self, sess, chars, vocab, num=200, prime='The ', sampling_type=1):
        state = sess.run(self.cell.zero_state(1, tf.float32))
        for char in prime[:-1]:
            x = np.zeros((1, 1))
            x[0, 0] = vocab[char]
            feed = {self.input_data: x, self.initial_state: state}
            [state] = sess.run([self.final_state], feed)

        def weighted_pick(weights):
            t = np.cumsum(weights)
            s = np.sum(weights)
            return(int(np.searchsorted(t, np.random.rand(1)*s)))

        ret = prime
        char = prime[-1]
        for n in range(num):
            x = np.zeros((1, 1))
            x[0, 0] = vocab[char]
            feed = {self.input_data: x, self.initial_state: state}
            [probs, state] = sess.run([self.probs, self.final_state], feed)
            p = probs[0]

            if sampling_type == 0:
                sample = np.argmax(p)
            elif sampling_type == 2:
                if char == ' ':
                    sample = weighted_pick(p)
                else:
                    sample = np.argmax(p)
            else:  # sampling_type == 1 default:
                sample = weighted_pick(p)

            pred = chars[sample]
            ret += pred
            char = pred
        return ret
```

- 音乐生成神经网络训练代码

```python
from __future__ import print_function
import tensorflow as tf

import argparse
import time
import os
from six.moves import cPickle

from utils import TextLoader
from model import Model

def main():
    parser = argparse.ArgumentParser(
                        formatter_class=argparse.ArgumentDefaultsHelpFormatter)
    parser.add_argument('--data_dir', type=str, default='data/tinyshakespeare',
                        help='data directory containing input.txt')
    parser.add_argument('--save_dir', type=str, default='save',
                        help='directory to store checkpointed models')
    parser.add_argument('--log_dir', type=str, default='logs',
                        help='directory to store tensorboard logs')
    parser.add_argument('--rnn_size', type=int, default=128,
                        help='size of RNN hidden state')
    parser.add_argument('--num_layers', type=int, default=2,
                        help='number of layers in the RNN')
    parser.add_argument('--model', type=str, default='lstm',
                        help='rnn, gru, lstm, or nas')
    parser.add_argument('--batch_size', type=int, default=50,
                        help='minibatch size')
    parser.add_argument('--seq_length', type=int, default=50,
                        help='RNN sequence length')
    parser.add_argument('--num_epochs', type=int, default=50,
                        help='number of epochs')
    parser.add_argument('--save_every', type=int, default=1000,
                        help='save frequency')
    parser.add_argument('--grad_clip', type=float, default=5.,
                        help='clip gradients at this value')
    parser.add_argument('--learning_rate', type=float, default=0.002,
                        help='learning rate')
    parser.add_argument('--decay_rate', type=float, default=0.97,
                        help='decay rate for rmsprop')
    parser.add_argument('--output_keep_prob', type=float, default=1.0,
                        help='probability of keeping weights in the hidden layer')
    parser.add_argument('--input_keep_prob', type=float, default=1.0,
                        help='probability of keeping weights in the input layer')
    parser.add_argument('--init_from', type=str, default=None,
                        help="""continue training from saved model at this path. Path must contain files saved by previous training process:
                            'config.pkl'        : configuration;
                            'chars_vocab.pkl'   : vocabulary definitions;
                            'checkpoint'        : paths to model file(s) (created by tf).
                                                  Note: this file contains absolute paths, be careful when moving files around;
                            'model.ckpt-*'      : file(s) with model definition (created by tf)
                        """)
    args = parser.parse_args()
    train(args)


def train(args):
    data_loader = TextLoader(args.data_dir, args.batch_size, args.seq_length)
    args.vocab_size = data_loader.vocab_size

    # check compatibility if training is continued from previously saved model
    if args.init_from is not None:
        # check if all necessary files exist
        assert os.path.isdir(args.init_from)," %s must be a a path" % args.init_from
        assert os.path.isfile(os.path.join(args.init_from,"config.pkl")),"config.pkl file does not exist in path %s"%args.init_from
        assert os.path.isfile(os.path.join(args.init_from,"chars_vocab.pkl")),"chars_vocab.pkl.pkl file does not exist in path %s" % args.init_from
        ckpt = tf.train.get_checkpoint_state(args.init_from)
        assert ckpt, "No checkpoint found"
        assert ckpt.model_checkpoint_path, "No model path found in checkpoint"

        # open old config and check if models are compatible
        with open(os.path.join(args.init_from, 'config.pkl'), 'rb') as f:
            saved_model_args = cPickle.load(f)
        need_be_same = ["model", "rnn_size", "num_layers", "seq_length"]
        for checkme in need_be_same:
            assert vars(saved_model_args)[checkme]==vars(args)[checkme],"Command line argument and saved model disagree on '%s' "%checkme

        # open saved vocab/dict and check if vocabs/dicts are compatible
        with open(os.path.join(args.init_from, 'chars_vocab.pkl'), 'rb') as f:
            saved_chars, saved_vocab = cPickle.load(f)
        assert saved_chars==data_loader.chars, "Data and loaded model disagree on character set!"
        assert saved_vocab==data_loader.vocab, "Data and loaded model disagree on dictionary mappings!"

    if not os.path.isdir(args.save_dir):
        os.makedirs(args.save_dir)
    with open(os.path.join(args.save_dir, 'config.pkl'), 'wb') as f:
        cPickle.dump(args, f)
    with open(os.path.join(args.save_dir, 'chars_vocab.pkl'), 'wb') as f:
        cPickle.dump((data_loader.chars, data_loader.vocab), f)

    model = Model(args)

    with tf.Session() as sess:
        # instrument for tensorboard
        summaries = tf.summary.merge_all()
        writer = tf.summary.FileWriter(
                os.path.join(args.log_dir, time.strftime("%Y-%m-%d-%H-%M-%S")))
        writer.add_graph(sess.graph)

        sess.run(tf.global_variables_initializer())
        saver = tf.train.Saver(tf.global_variables())
        # restore model
        if args.init_from is not None:
            saver.restore(sess, ckpt.model_checkpoint_path)
        for e in range(args.num_epochs):
            sess.run(tf.assign(model.lr,
                               args.learning_rate * (args.decay_rate ** e)))
            data_loader.reset_batch_pointer()
            state = sess.run(model.initial_state)
            for b in range(data_loader.num_batches):
                start = time.time()
                x, y = data_loader.next_batch()
                feed = {model.input_data: x, model.targets: y}
                for i, (c, h) in enumerate(model.initial_state):
                    feed[c] = state[i].c
                    feed[h] = state[i].h

                # instrument for tensorboard
                summ, train_loss, state, _ = sess.run([summaries, model.cost, model.final_state, model.train_op], feed)
                writer.add_summary(summ, e * data_loader.num_batches + b)

                end = time.time()
                print("{}/{} (epoch {}), train_loss = {:.3f}, time/batch = {:.3f}"
                      .format(e * data_loader.num_batches + b,
                              args.num_epochs * data_loader.num_batches,
                              e, train_loss, end - start))
                if (e * data_loader.num_batches + b) % args.save_every == 0\
                        or (e == args.num_epochs-1 and
                            b == data_loader.num_batches-1):
                    # save for the last result
                    checkpoint_path = os.path.join(args.save_dir, 'model.ckpt')
                    saver.save(sess, checkpoint_path,
                               global_step=e * data_loader.num_batches + b)
                    print("model saved to {}".format(checkpoint_path))
```

- 音乐生成神经网络采样部分代码

```python
from __future__ import print_function
import tensorflow as tf
import os
from six.moves import cPickle
import re
from model import Model

class Generation:
    def __init__(self, file_name, v, a):
        self.save_dir = "./model"
        self.file_name = file_name
        self.valence = v
        self.arousal = a
        self.n = 500
        self.prime = self.calculate()
        self.raw = ""

    def calculate(self):
        return generatePrime(self.v, self.a)

    def sample(self):
        with open(os.path.join(self.save_dir, 'config.pkl'), 'rb') as f:
            saved_args = cPickle.load(f)
        with open(os.path.join(self.save_dir, 'chars_vocab.pkl'), 'rb') as f:
            chars, vocab = cPickle.load(f)
        model = Model(saved_args, training=False)
        with tf.Session() as sess:
            tf.global_variables_initializer().run()
            saver = tf.train.Saver(tf.global_variables())
            ckpt = tf.train.get_checkpoint_state(self.save_dir)
            if ckpt and ckpt.model_checkpoint_path:
                saver.restore(sess, ckpt.model_checkpoint_path)
                self.raw = model.sample(sess, chars, vocab, self.n, self.prime, 1).encode('utf-8')

    def deal_abc(self):
        print(self.raw)
        t = str(self.raw)
        idx = t.index('@X:')
        print(idx)
        text = t[2:idx]
        strinfo = re.compile('@')
        abc = strinfo.sub('\n', text)
        p = os.getcwd()
        f = open(p+'/tmp.abc', 'w')

        f.write(abc)
        f.close()
        os.system("abc2midi "+p+"/tmp.abc -o "+self.file_name)
        os.remove(p+'/tmp.abc')
```

- 软件图形界面主窗口代码

```python
from PyQt5 import QtCore, QtGui, QtWidgets

class Ui_MainWindow(object):
    def setupUi(self, MainWindow):
        MainWindow.setObjectName("MainWindow")
        MainWindow.resize(538, 375)
        sizePolicy = QtWidgets.QSizePolicy(QtWidgets.QSizePolicy.Preferred, QtWidgets.QSizePolicy.Preferred)
        sizePolicy.setHorizontalStretch(0)
        sizePolicy.setVerticalStretch(0)
        sizePolicy.setHeightForWidth(MainWindow.sizePolicy().hasHeightForWidth())
        MainWindow.setSizePolicy(sizePolicy)
        self.centralWidget = QtWidgets.QWidget(MainWindow)
        self.centralWidget.setObjectName("centralWidget")
        self.gridLayout = QtWidgets.QGridLayout(self.centralWidget)
        self.gridLayout.setContentsMargins(11, 11, 11, 11)
        self.gridLayout.setSpacing(6)
        self.gridLayout.setObjectName("gridLayout")
        self.verticalLayout_3 = QtWidgets.QVBoxLayout()
        self.verticalLayout_3.setSizeConstraint(QtWidgets.QLayout.SetNoConstraint)
        self.verticalLayout_3.setSpacing(6)
        self.verticalLayout_3.setObjectName("verticalLayout_3")
        self.horizontalLayout = QtWidgets.QHBoxLayout()
        self.horizontalLayout.setSpacing(6)
        self.horizontalLayout.setObjectName("horizontalLayout")
        self.pushButton_4 = QtWidgets.QPushButton(self.centralWidget)
        sizePolicy = QtWidgets.QSizePolicy(QtWidgets.QSizePolicy.Expanding, QtWidgets.QSizePolicy.Expanding)
        sizePolicy.setHorizontalStretch(0)
        sizePolicy.setVerticalStretch(1)
        sizePolicy.setHeightForWidth(self.pushButton_4.sizePolicy().hasHeightForWidth())
        self.pushButton_4.setSizePolicy(sizePolicy)
        font = QtGui.QFont()
        font.setFamily("Iosevka")
        font.setPointSize(14)
        self.pushButton_4.setFont(font)
        self.pushButton_4.setObjectName("pushButton_4")
        self.horizontalLayout.addWidget(self.pushButton_4)
        self.pushButton_3 = QtWidgets.QPushButton(self.centralWidget)
        sizePolicy = QtWidgets.QSizePolicy(QtWidgets.QSizePolicy.Expanding, QtWidgets.QSizePolicy.Expanding)
        sizePolicy.setHorizontalStretch(0)
        sizePolicy.setVerticalStretch(1)
        sizePolicy.setHeightForWidth(self.pushButton_3.sizePolicy().hasHeightForWidth())
        self.pushButton_3.setSizePolicy(sizePolicy)
        font = QtGui.QFont()
        font.setFamily("Iosevka")
        font.setPointSize(14)
        self.pushButton_3.setFont(font)
        self.pushButton_3.setObjectName("pushButton_3")
        self.horizontalLayout.addWidget(self.pushButton_3)
        self.verticalLayout_3.addLayout(self.horizontalLayout)
        self.horizontalLayout_2 = QtWidgets.QHBoxLayout()
        self.horizontalLayout_2.setSpacing(6)
        self.horizontalLayout_2.setObjectName("horizontalLayout_2")
        self.verticalLayout = QtWidgets.QVBoxLayout()
        self.verticalLayout.setSpacing(6)
        self.verticalLayout.setObjectName("verticalLayout")
        self.label = QtWidgets.QLabel(self.centralWidget)
        sizePolicy = QtWidgets.QSizePolicy(QtWidgets.QSizePolicy.Expanding, QtWidgets.QSizePolicy.Expanding)
        sizePolicy.setHorizontalStretch(0)
        sizePolicy.setVerticalStretch(1)
        sizePolicy.setHeightForWidth(self.label.sizePolicy().hasHeightForWidth())
        self.label.setSizePolicy(sizePolicy)
        font = QtGui.QFont()
        font.setFamily("Iosevka")
        font.setPointSize(14)
        self.label.setFont(font)
        self.label.setAlignment(QtCore.Qt.AlignCenter)
        self.label.setObjectName("label")
        self.verticalLayout.addWidget(self.label)
        self.lcdNumber_2 = QtWidgets.QLCDNumber(self.centralWidget)
        sizePolicy = QtWidgets.QSizePolicy(QtWidgets.QSizePolicy.Expanding, QtWidgets.QSizePolicy.Expanding)
        sizePolicy.setHorizontalStretch(0)
        sizePolicy.setVerticalStretch(3)
        sizePolicy.setHeightForWidth(self.lcdNumber_2.sizePolicy().hasHeightForWidth())
        self.lcdNumber_2.setSizePolicy(sizePolicy)
        font = QtGui.QFont()
        font.setFamily("Iosevka")
        font.setPointSize(14)
        self.lcdNumber_2.setFont(font)
        self.lcdNumber_2.setObjectName("lcdNumber_2")
        self.verticalLayout.addWidget(self.lcdNumber_2)
        self.horizontalLayout_2.addLayout(self.verticalLayout)
        self.verticalLayout_2 = QtWidgets.QVBoxLayout()
        self.verticalLayout_2.setSpacing(6)
        self.verticalLayout_2.setObjectName("verticalLayout_2")
        self.label_2 = QtWidgets.QLabel(self.centralWidget)
        sizePolicy = QtWidgets.QSizePolicy(QtWidgets.QSizePolicy.Expanding, QtWidgets.QSizePolicy.Expanding)
        sizePolicy.setHorizontalStretch(0)
        sizePolicy.setVerticalStretch(1)
        sizePolicy.setHeightForWidth(self.label_2.sizePolicy().hasHeightForWidth())
        self.label_2.setSizePolicy(sizePolicy)
        font = QtGui.QFont()
        font.setFamily("Iosevka")
        font.setPointSize(14)
        self.label_2.setFont(font)
        self.label_2.setMouseTracking(True)
        self.label_2.setTextFormat(QtCore.Qt.AutoText)
        self.label_2.setAlignment(QtCore.Qt.AlignCenter)
        self.label_2.setObjectName("label_2")
        self.verticalLayout_2.addWidget(self.label_2)
        self.lcdNumber = QtWidgets.QLCDNumber(self.centralWidget)
        sizePolicy = QtWidgets.QSizePolicy(QtWidgets.QSizePolicy.Expanding, QtWidgets.QSizePolicy.Expanding)
        sizePolicy.setHorizontalStretch(0)
        sizePolicy.setVerticalStretch(3)
        sizePolicy.setHeightForWidth(self.lcdNumber.sizePolicy().hasHeightForWidth())
        self.lcdNumber.setSizePolicy(sizePolicy)
        font = QtGui.QFont()
        font.setFamily("Iosevka")
        font.setPointSize(14)
        self.lcdNumber.setFont(font)
        self.lcdNumber.setObjectName("lcdNumber")
        self.verticalLayout_2.addWidget(self.lcdNumber)
        self.horizontalLayout_2.addLayout(self.verticalLayout_2)
        self.horizontalLayout_2.setStretch(0, 1)
        self.horizontalLayout_2.setStretch(1, 1)
        self.verticalLayout_3.addLayout(self.horizontalLayout_2)
        self.verticalLayout_3.setStretch(0, 1)
        self.verticalLayout_3.setStretch(1, 4)
        self.gridLayout.addLayout(self.verticalLayout_3, 0, 0, 1, 1)
        MainWindow.setCentralWidget(self.centralWidget)
        self.menuBar = QtWidgets.QMenuBar(MainWindow)
        self.menuBar.setGeometry(QtCore.QRect(0, 0, 538, 22))
        self.menuBar.setObjectName("menuBar")
        MainWindow.setMenuBar(self.menuBar)
        self.mainToolBar = QtWidgets.QToolBar(MainWindow)
        self.mainToolBar.setObjectName("mainToolBar")
        MainWindow.addToolBar(QtCore.Qt.TopToolBarArea, self.mainToolBar)
        self.statusBar = QtWidgets.QStatusBar(MainWindow)
        self.statusBar.setObjectName("statusBar")
        MainWindow.setStatusBar(self.statusBar)

        self.retranslateUi(MainWindow)
        self.pushButton_4.clicked.connect(MainWindow.click_recognize)
        self.pushButton_3.clicked.connect(MainWindow.click_generate)
        QtCore.QMetaObject.connectSlotsByName(MainWindow)

    def retranslateUi(self, MainWindow):
        _translate = QtCore.QCoreApplication.translate
        MainWindow.setWindowTitle(_translate("MainWindow", "MainWindow"))
        self.pushButton_4.setText(_translate("MainWindow", "Recognize"))
        self.pushButton_3.setText(_translate("MainWindow", "Generate"))
        self.label.setText(_translate("MainWindow", "Valence"))
        self.label_2.setText(_translate("MainWindow", "Arousal"))
```

- 软件图形界面实现、生成功能代码部分（软件入口）

```python
import sys
import os
from PyQt5.QtCore import *
from PyQt5.QtWidgets import *
from mainwindow import Ui_MainWindow
from PyQt5 import QtWidgets
from Demo import Classification
from PyQt5.QtGui import *
from sample import Generation

inputFile = ""
openSmilePath = "/Users/xwy/Downloads/openSMILE-2.1.0/"
smileExtract = openSmilePath + "inst/bin/SMILExtract"
configPath = "./IS10_paraling_2.conf"


class MyWindow(QtWidgets.QMainWindow, Ui_MainWindow):
    def __init__(self):
        super(MyWindow, self).__init__()
        self.setupUi(self)
        self.temp_count = 0
        self.open_filename = ""
        self.open_filetype = ""
        self.save_filename = ""
        self.save_filetype = ""
        self.m = MultiInPutDialog()
        self.m.btn.installEventFilter(self)

    def click_recognize(self):
        self.open_filename, self.open_filetype = QFileDialog.getOpenFileName(self, "Open a MP3 file", "/",
                                                                             "MP3 (*.mp3)")
        print(self.open_filename)
        if self.open_filename:
            if os.path.exists("out.csv"):
                os.remove("out.csv")
            if os.path.exists("out.wav"):
                os.remove("out.wav")
            ret = os.system("ffmpeg -i " + self.open_filename + " -vn out.wav &> info.txt")
            os.system(smileExtract + " -C " + configPath + " -I out.wav -O out.csv &> info.txt")
            c = Classification("out.csv")
            res = c.run()
            if (res == 1):
                self.lcdNumber_2.display(c.valance * 10)
                self.lcdNumber.display(c.arousal * 10)
        pass

    def click_generate(self):
        self.save_filename, self.save_filetype = QFileDialog.getSaveFileName(self, "Save a MIDI file", "/",
                                                                             "MIDI (*.mid);")
        print(self.save_filename)
        self.m.btn.setFileName(self.save_filename)
        self.installEventFilter(self.m)
        self.m.focusWidget()
        if self.m.exec_():
            print("ok")

    def eventFilter(self, source, event):
        if source == self.m.btn:
            if event.type() == QEvent.MouseMove:
                pos = event.pos()
                self.m.vtxt.setText('x:%d, y:%d' % (pos.x(),  pos.y()))
        return QMainWindow.eventFilter(self,  source,  event)


class MultiInPutDialog(QDialog):
    def __init__(self, parent=None):
        QDialog.__init__(self, parent)
        self.resize(500, 600)
        self.setWindowTitle('Set V-A Value')
        self.btn = VAButton()
        layout = QVBoxLayout()
        btnLayout = QHBoxLayout()
        btnLayout.addWidget(self.btn)
        layout.addStretch()
        layout.addItem(btnLayout)
        hlayout = QHBoxLayout()
        hlayout.addWidget(QLabel("Valence:"))
        self.vtxt = QLabel()
        hlayout.addWidget(self.vtxt)
        hlayout.addWidget(QLabel("Arousal:"))
        self.atxt = QLabel()
        hlayout.addWidget(self.atxt)
        layout.addStretch()
        layout.addItem(hlayout)
        layout.addStretch()
        self.setLayout(layout)

class VAButton(QPushButton):
    def __init__(self, parent=None):
        super(VAButton, self).__init__(parent)
        self.file_name = ""
        self.hovered = False
        self.pressed = False
        self.pixmap = QPixmap("va.png")
        self.roundness = 0
        rect = QRect()
        rect.setWidth(300)
        rect.setHeight(300)
        self.setGeometry(rect)
        self.setFixedSize(QSize(300, 300))
        self.setIconSize(QSize(300, 300))

    def setFileName(self, file_name):
        self.file_name = file_name

    def paintEvent(self, event):
        painter = QPainter(self)
        painter.setRenderHint(QPainter.Antialiasing)

        button_rect = QRect(self.geometry())
        # button_rect.setSize(QSize(100, 100))
        painter.setPen(QPen(QBrush(Qt.red), 2.0))
        painter_path = QPainterPath()
        painter_path.addRoundedRect(1, 1, button_rect.width() - 2, button_rect.height() - 2, self.roundness,
                                    self.roundness)
        painter.setClipPath(painter_path)
        if self.isEnabled():
            icon_size = self.iconSize()
            icon_position = self.calculateIconPosition(button_rect, icon_size)
            painter.setOpacity(1.0)
            # painter.drawRect(icon_position)
            painter.drawPixmap(icon_position, self.pixmap)

    def enterEvent(self, event):
        self.hovered = True
        self.repaint()
        QPushButton.enterEvent(self, event)

    def leaveEvent(self, event):
        self.hovered = False
        self.repaint()
        QPushButton.leaveEvent(self, event)

    def mousePressEvent(self, event):
        self.pressed = True
        self.repaint()
        g = Generation(self.file_name, 1, 1)
        g.calculate()
        g.sample()
        g.deal_abc()
        QPushButton.mousePressEvent(self, event)

    def mouseReleaseEvent(self, event):
        self.pressed = False
        self.repaint()
        QPushButton.mouseReleaseEvent(self, event)

    def calculateIconPosition(self, button_rect, icon_size):
        x = (button_rect.width() / 2) - (icon_size.width() / 2)
        y = (button_rect.height() / 2) - (icon_size.height() / 2)
        width = icon_size.width()
        height = icon_size.height()
        icon_position = QRect()
        icon_position.setX(x)
        icon_position.setY(y)
        icon_position.setWidth(width)
        icon_position.setHeight(height)
        return icon_position

if __name__ == '__main__':
    app = QApplication(sys.argv)
    widget = MyWindow()
    widget.show()
    app.installEventFilter(widget)
    sys.exit(app.exec_())
```

- 软件图形界面音乐分类部分逻辑代码

```python
import numpy as np
import pickle as pkl
from keras import models


class Classification:
    def __init__(self, csvName):
        self.data_x_file = csvName
        self.batch_size = 64
        self.mean = pkl.load(open("mean", "rb"))
        self.std = pkl.load(open("std", "rb"))
        self.model_A = models.load_model("model_adagrad_A_103")
        self.model_V = models.load_model("model_adagrad_V_102")
        self.valance = 0
        self.arousal = 0
        
    def run(self):
        my_matrix = np.loadtxt(open(self.data_x_file, "rb"), delimiter=';', skiprows=1)
        my_matrix = np.delete(my_matrix, 0, 1)
        data_x = []
        if len(my_matrix) < 218 + 75:
            print("The song is too short, we need a song with at least 60 seconds.")
            return 0
        for i in range(self.batch_size):
            data_x.append(my_matrix[76:292, :])
        data_x -= self.mean
        data_x /= self.std

        classes_V = self.model_V.predict(data_x[0:64, :, :], batch_size=self.batch_size)
        classes_A = self.model_A.predict(data_x[0:64, :, :], batch_size=self.batch_size)
        self.valance = (classes_V[0] - 0.5) * 1.2 + 0.5
        self.arousal = (classes_A[0] - 0.5) * 1.2 + 0.5
        return 1
```

- 音乐数据处理代码

```python
# -*- coding: utf-8 -*-
import pandas as pd
import numpy as np
import pickle as pkl

#read into a dataframe

csv_col=[]

label_file = "annotations/annotations averaged per song/dynamic (per second annotations)/song_id.csv"
label = pd.read_csv(label_file)
label = label["song_id"].tolist()

data_x = []
song_number = label#label[:, 0]

print (len(song_number))
for i in song_number:
    
    if i % 100 == 0:
        print ("Reading " + str(int(i)))
    my_matrix = np.loadtxt(open('csv/' + str(int(i)) + '.csv', "rb"), delimiter=';', skiprows=1)
    my_matrix = np.delete(my_matrix, 0, 1)
    list_ma = [j for j in range(0,76) if j%5==0]
    data_x.append(my_matrix[list_ma, :])
    for k in range(5,141):    # 0 5 10 15 20 ... 75   5  28*5+1   
        if k % 5 == 0:
            list_ma=[x+5 for x in list_ma]
            data_x.append(my_matrix[list_ma, :]) #i from 2 to 56       
#        
print ((np.array(data_x).shape))
data_x -= np.mean(data_x, axis=0)
data_x /= np.std(data_x, axis=0)
pkl.dump(data_x[0:10000,:], open("data_x_new1", "wb"))
pkl.dump(data_x[10000:,:], open("data_x_new2", "wb"))

```

- 音乐分类模型代码

```python
import numpy as np
import random
import matplotlib.pyplot as plt
import pickle as pkl

# from IPython import embed

from pprint import pprint
from keras.models import Sequential, load_model
from keras.layers.recurrent import LSTM, SimpleRNN
from keras.layers.normalization import BatchNormalization
from keras.layers.noise import GaussianNoise
from keras.layers import Dense, Activation
from keras.optimizers import Adam, RMSprop, SGD, Adagrad
from keras import backend as K
from math import sqrt
import pandas as pd
from sklearn import preprocessing
import os

def sqr(x):
    return x * x

data_x = []
data_dir = "features"


train_data_size = 10000
batch_size = 128
nb_epochs = 15
early_stop_n = 40

opt = SGD(lr = 0.2, momentum = 0.1, decay = 0.05, nesterov = True)
train_data_x = []
train_data_y = []
test_data_x = []
test_data_y = []

label_file = "annotations/annotations averaged per song/dynamic (per second annotations)/song_id.csv"
label = pd.read_csv(label_file)
label = label["song_id"].tolist()
csv_all = pd.read_csv(open("annotations/annotations averaged per song/dynamic (per second annotations)/arousal2.csv",'rb'),index_col=0)

csv_col=[]
for i in range(58):  #y from 15 to 43s last sec for measurement
    if i%2 == 0:
        csv_col.append(csv_all.columns[i])
#print (csv_col) 

csv_sec = csv_all[csv_col]

csv_list_y = []
data_y = []

for i in label:
    csv_list_y = csv_sec[csv_col].loc[i].tolist()
    data_y.extend(csv_list_y)
 
print (len(data_y))

data_y_v = []
csv_all_v = pd.read_csv(open("annotations/annotations averaged per song/dynamic (per second annotations)/valence3.csv",'rb'),index_col=0)
print (csv_all_v.columns) # =1224
csv_col_v=[]
for i in range(58):  #y from 15 to 43s
    if i%2 == 0:
        csv_col_v.append(csv_all_v.columns[i])

csv_sec_v = csv_all_v[csv_col_v]
#csv_sec_v = csv_sec_v + 1/2
csv_list_y_v = []
data_y_v = []
for i in label:
    csv_list_y_v = csv_sec[csv_col_v].loc[i].tolist()
    data_y_v.extend(csv_list_y_v)


data_y = np.array([data_y])

data_y = np.transpose(data_y, (1, 0))
print("data_y shape ",data_y.shape)

data_y = [i/10 for i in data_y]


data_x1 = pd.read_pickle('data_x_new1')
data_x2 = pd.read_pickle('data_x_new2')

data_x = np.concatenate([data_x1,data_x2])

print ('data_x shape',data_x[0].shape)  #(1744,87,260)
#print ('data_y shape',data_y_new.shape)
print ("Randomly dividing data into train set and test set...")
number = np.arange(len(data_x))
print ('number=',number)  #[0 1 2 ..., 1741 1742 1743]
train_number = random.sample(set(number), train_data_size)
test_number = set(number).difference(set(train_number))
print ('train number',len(train_number),'test_number',len(test_number))     
print ("Train Set Generating... ")
for i in train_number:
    train_data_x.append(data_x[i])
    train_data_y.append(data_y[i])
print ("Test Set Generating...")
for i in test_number:
    test_data_x.append(data_x[i])
    test_data_y.append(data_y[i])

data_x = np.array(data_x)
data_y = np.array(data_y)
test_data_x = np.array(test_data_x)
test_data_x = test_data_x[0:1024, :, :] 
test_data_y = np.array(test_data_y)

test_data_y = test_data_y[0:1024]
train_data_x = np.array(train_data_x)
train_data_y = np.array(train_data_y)

print('Build LSTM RNN model ...')    #52258
print ('data_x.shape[1:]=',data_x.shape[1:])  #(87,260) (260 features, 87 time sets)
model = Sequential()
model.add(GaussianNoise(input_shape=data_x.shape[1:], stddev=0.05))
model.add(LSTM(input_shape=data_x.shape[1:], #column number
               batch_size=batch_size, units=1000, dropout=0.15, recurrent_dropout=0.1, return_sequences=True))
model.add(LSTM(input_shape=data_x.shape[1:],batch_size=batch_size, units=1000, dropout=0.15, recurrent_dropout=0.15, return_sequences=True))
model.add(LSTM(input_shape=data_x.shape[1:],batch_size=batch_size, units=1000, dropout=0.15, recurrent_dropout=0.15, return_sequences=True))
model.add(LSTM(input_shape=data_x.shape[1:],batch_size=batch_size, units=1000, dropout=0.15, recurrent_dropout=0.15, return_sequences=True))
model.add(LSTM(activation='tanh', dropout=0.15, recurrent_dropout=0.1, return_sequences=False,
               units=10))
model.add(Dense(units=1, activation='tanh'))

print("Compiling ...")

def my_loss(y_true, y_pred):
    return K.maximum(K.mean(K.dot(K.reshape(y_true - y_pred, [1, 2]), K.reshape(y_true - y_pred, [2, 1]))) - 0.01, 0)

def my_mse(y_true, y_pred):
    return K.maximum(K.mean(K.square(y_true - y_pred)) - 0.005, 0)

model.compile(loss='mse', optimizer=opt, metrics=['accuracy'])
model.summary()


print("Training ...")

best_score = 1000
best_epoch = 0

losses = []
r2s = []
tloss = []
plt.title('LSTM Music Emotion Recognition Model - Valence')
plt.xlabel('Epoch')
plt.ylabel('MSE')

for i in range(1, 200):
    log_file = open("./Adagrad_log/" + str(i) + ".txt", "w")
    log_file.write("epoch " + str(i))
    print("epoch ", i)
   
    model.fit(train_data_x, train_data_y, batch_size=batch_size, epochs=1)
    model.save("models/model_" + str(i))

    
    score, accuracy = model.evaluate(test_data_x, np.array(test_data_y), batch_size=batch_size, verbose=1)
    log_file.write("Validation loss:  " + str(score))
    log_file.write("Validation accuracy:  " + str(accuracy))
    print("Validation loss:" + str(score))
    print("Validation accuracy:" + str(accuracy))
    
    if best_epoch == 0:
        best_epoch = 1
        best_score = score
    if score <= best_score:
        best_score = score
        best_epoch = i
    elif (score > best_score) and (i - best_epoch > early_stop_n):
        print("Early stop:", early_stop_n, "epochs.")
        print("Best epoch is", best_epoch)
        print("Best loss is", best_score)
        break
    print("Best loss:", best_epoch, best_score)

  
    classes = model.predict(test_data_x, batch_size = batch_size)
    class_df = pd.Series(classes.flatten())

    classes1 = model.predict(test_data_x[256:512, :, :], batch_size=batch_size)
    classes = np.concatenate((classes, classes1))

    
    
    correct_x = []
    correct_y = []
    error_x = []
    error_y = []
    correct = 0
    error = 0
    sse = 0
    for i in range(len(test_data_x)):
        dx = test_data_y[i][0] - classes[i][0]
        dy = test_data_y[i][1] - classes[i][1]
        sse += dx * dx + dy * dy
        if sqrt(dx * dx + dy * dy) < 0.01:
            correct = correct + 1
            correct_x.append(classes[i][0])
            correct_y.append(classes[i][1])
            
        else:
            error = error + 1
            error_x.append(classes[i][0])
            error_y.append(classes[i][1])
        

    log_file.write("correct: " + str(len(correct_y)) + "\nerror: " + str(len(error_x)))
    print("correct: " + str(len(correct_y)) + "\nerror: " + str(len(error_x)))
    mse = sse / len(test_data_x)
    print("mse = ", mse)
    ratio = correct/(correct+error)
    print ('ratio:%.2f'%(ratio))
    print (' correct:%d'%(correct)+' error:%d'%(error))
   
    losses.append(score)
    score, accuracy = model.evaluate(train_data_x, np.array(train_data_y), batch_size=batch_size, verbose=0)
    print("Train loss:", score)
    tloss.append(score)
    np.savetxt("loss.txt", losses)
    np.savetxt("r2.txt", r2s)
  
    log_file.close()
    plt.scatter(correct_x, correct_y, c='g', marker='.')
    plt.scatter(error_x, error_y, c='r', marker='.')
    plt.show()
   
    if (i > 1):
        plt.plot([j for j in range(2, i + 1)], losses[1:], 'ro--', label='validation loss')
        plt.plot([j for j in range(2, i + 1)], tloss[1:], 'bo--', label='train loss')
    plt.savefig("loss.png")
```

- 读取音乐midi文件信息代码

```python
import midi
import os
import math
import pickle as pkl
import numpy as np

# INDICES IN BATCHES (LENGTH,FREQ,VELOCITY are repeated self.tones_per_cell times):
TICKS_FROM_PREV_START = 0
LENGTH = 1
FREQ = 2
VELOCITY = 3
tones_per_cell = 1

# INDICES IN SONG DATA (NOT YET BATCHED):
BEGIN_TICK = 0
NUM_FEATURES_PER_TONE = 3
output_ticks_per_quarter_note = 384.0


def read_one_file(path, filename, pace_events):
    print('Reading {}'.format(os.path.join(path, filename)))
    midi_pattern = midi.read_midifile(os.path.join(path, filename))
    song_data = []
    # Tempo:
    ticks_per_quarter_note = float(midi_pattern.resolution)
    print('Resoluton: {}'.format(ticks_per_quarter_note))
    input_ticks_per_output_tick = ticks_per_quarter_note / output_ticks_per_quarter_note
    # if debug == 'overfit': input_ticks_per_output_tick = 1.0

    # Multiply with output_ticks_pr_input_tick for output ticks.
    for track in midi_pattern:
        last_event_input_tick = 0
        not_closed_notes = []
        for event in track:
            if type(event) == midi.events.SetTempoEvent:
                pass  # These are currently ignored
            elif (type(event) == midi.events.NoteOffEvent) or \
                    (type(event) == midi.events.NoteOnEvent and \
                                 event.velocity == 0):
                retained_not_closed_notes = []
                for e in not_closed_notes:
                    if tone_to_freq(event.data[0]) == e[FREQ]:
                        event_abs_tick = float(event.tick + last_event_input_tick) / input_ticks_per_output_tick
                        # current_note['length'] = float(ticks*microseconds_per_tick)
                        e[LENGTH] = event_abs_tick - e[BEGIN_TICK]
                        song_data.append(e)
                    else:
                        retained_not_closed_notes.append(e)
                if len(not_closed_notes) == len(retained_not_closed_notes):
                	print('Warning. NoteOffEvent, but len(not_closed_notes)({}) == len(retained_not_closed_notes)({})'.format(len(not_closed_notes), len(retained_not_closed_notes)))
                    print('NoteOff: {}'.format(tone_to_freq(event.data[0])))
                    print('not closed: {}'.format(not_closed_notes))
                not_closed_notes = retained_not_closed_notes
            elif type(event) == midi.events.NoteOnEvent:
                begin_tick = float(event.tick + last_event_input_tick) / input_ticks_per_output_tick
                note = [0.0] * (NUM_FEATURES_PER_TONE + 1)
                note[FREQ] = tone_to_freq(event.data[0])
                note[VELOCITY] = float(event.data[1])
                note[BEGIN_TICK] = begin_tick
                not_closed_notes.append(note)
                not_closed_notes.append([0.0, tone_to_freq(event.data[0]), velocity, begin_tick, event.channel])
            last_event_input_tick += event.tick
        for e in not_closed_notes:
            print('Warning: found no NoteOffEvent for this note. Will close it. {}'.format(e))
            e[LENGTH] = float(ticks_per_quarter_note) / input_ticks_per_output_tick
            song_data.append(e)
    song_data.sort(key=lambda e: e[BEGIN_TICK])
    if (pace_events):
        pace_event_list = []
        pace_tick = 0.0
        song_tick_length = song_data[-1][BEGIN_TICK] + song_data[-1][LENGTH]
        while pace_tick < song_tick_length:
            song_data.append([0.0, 440.0, 0.0, pace_tick, 0.0])
            pace_tick += float(ticks_per_quarter_note) / input_ticks_per_output_tick
        song_data.sort(key=lambda e: e[BEGIN_TICK])
    song_data.sort(key=lambda e: e[BEGIN_TICK])
    return song_data


def tone_to_freq(tone):
    return math.pow(2, ((float(tone) - 69.0) / 12.0)) * 440.0


def get_midi_pattern(song_data):
    """
    returns the midi_pattern.
    Can be used with filename == None. Then nothing is saved, but only returned.
    """
    print('song_data[0:10]: {}'.format(song_data[0:10]))

    midi_pattern = midi.Pattern([], resolution=int(output_ticks_per_quarter_note))
    cur_track = midi.Track([])
    cur_track.append(midi.events.SetTempoEvent(tick=0, bpm=45))
    future_events = {}
    last_event_tick = 0

    ticks_to_this_tone = 0.0
    song_events_absolute_ticks = []
    abs_tick_note_beginning = 0.0
    for frame in song_data:
        abs_tick_note_beginning += frame[TICKS_FROM_PREV_START]
        for subframe in xrange(tones_per_cell):
            offset = subframe * NUM_FEATURES_PER_TONE
            tick_len = int(round(frame[offset + LENGTH]))
            freq = frame[offset + FREQ]
            velocity = min(int(round(frame[offset + VELOCITY])), 127)
            print('tick_len: {}, freq: {}, velocity: {}, ticks_from_prev_start: {}'.format(tick_len, freq, velocity, frame[TICKS_FROM_PREV_START]))
            d = freq_to_tone(freq)
            print('d: {}'.format(d))
            if d is not None and velocity > 0 and tick_len > 0:
                # range-check with preserved tone, changed one octave:
                tone = d['tone']
                while tone < 0:   tone += 12
                while tone > 127: tone -= 12
                pitch_wheel = cents_to_pitchwheel_units(d['cents'])
                print('tick_len: {}, freq: {}, tone: {}, pitch_wheel: {}, velocity: {}'.format(tick_len, freq, tone,
                                                                                               pitch_wheel, velocity))
                if pitch_wheel != 0:
                    midi.events.PitchWheelEvent(tick=int(ticks_to_this_tone),
                                                pitch=pitch_wheel)
                song_events_absolute_ticks.append((abs_tick_note_beginning,
                                                   midi.events.NoteOnEvent(
                                                       tick=0,
                                                       velocity=velocity,
                                                       pitch=tone)))
                song_events_absolute_ticks.append((abs_tick_note_beginning + tick_len,
                                                   midi.events.NoteOffEvent(
                                                       tick=0,
                                                       velocity=0,
                                                       pitch=tone)))
    song_events_absolute_ticks.sort(key=lambda e: e[0])
    abs_tick_note_beginning = 0.0
    for abs_tick, event in song_events_absolute_ticks:
        rel_tick = abs_tick - abs_tick_note_beginning
        event.tick = int(round(rel_tick))
        cur_track.append(event)
        abs_tick_note_beginning = abs_tick

    cur_track.append(midi.EndOfTrackEvent(tick=int(output_ticks_per_quarter_note)))
    midi_pattern.append(cur_track)
    print 'Printing midi track.'
    print midi_pattern
    return midi_pattern


def cents_to_pitchwheel_units(cents):
    return int(40.96 * (float(cents)))


def freq_to_tone(freq):
    """
      returns a dict d where
      d['tone'] is the base tone in midi standard
      d['cents'] is the cents to make the tone into the exact-ish frequency provided.
                 multiply this with 8192 to get the midi pitch level.
    """
    if freq <= 0.0:
        return None
    float_tone = (69.0 + 12 * math.log(float(freq) / 440.0, 2))
    int_tone = int(float_tone)
    cents = int(1200 * math.log(float(freq) / tone_to_freq(int_tone), 2))
    return {'tone': int_tone, 'cents': cents}


def get_midi_pattern(song_data):
    """
    get_midi_pattern takes a song in internal representation
    (a tensor of dimensions [songlength, self.num_song_features]).
    the three values are length, frequency, velocity.
    if velocity of a frame is zero, no midi event will be
    triggered at that frame.

    returns the midi_pattern.

    Can be used with filename == None. Then nothing is saved, but only returned.
    """
    print('song_data[0:10]: {}'.format(song_data[0:10]))

    #
    # Interpreting the midi pattern.
    # A pattern has a list of tracks
    # (midi.Track()).
    # Each track is a list of events:
    #     - data[0] is the note (0-127)
    #     - data[1] is the velocity.
    #     - if velocity is 0, this is equivalent of a midi.NoteOffEvent
    # Ticks are relative.
    # Tempo are in microseconds/quarter note.

    # Tempo:
    # Multiply with output_ticks_pr_input_tick for output ticks.
    midi_pattern = midi.Pattern([], resolution=int(output_ticks_per_quarter_note))
    cur_track = midi.Track([])
    cur_track.append(midi.events.SetTempoEvent(tick=0, bpm=45))
    future_events = {}
    last_event_tick = 0

    ticks_to_this_tone = 0.0
    song_events_absolute_ticks = []
    abs_tick_note_beginning = 0.0
    for frame in song_data:
        abs_tick_note_beginning += frame[TICKS_FROM_PREV_START]
        for subframe in xrange(tones_per_cell):
            offset = subframe * NUM_FEATURES_PER_TONE
            tick_len = int(round(frame[offset + LENGTH]))
            freq = frame[offset + FREQ]
            velocity = min(int(round(frame[offset + VELOCITY])), 127)
            print('tick_len: {}, freq: {}, velocity: {}, ticks_from_prev_start: {}'.format(tick_len, freq, velocity, frame[TICKS_FROM_PREV_START]))
            d = freq_to_tone(freq)
            print('d: {}'.format(d))
            if d is not None and velocity > 0 and tick_len > 0:
                # range-check with preserved tone, changed one octave:
                tone = d['tone']
                while tone < 0:   tone += 12
                while tone > 127: tone -= 12
                pitch_wheel = cents_to_pitchwheel_units(d['cents'])
                print('tick_len: {}, freq: {}, tone: {}, pitch_wheel: {}, velocity: {}'.format(tick_len, freq, tone, pitch_wheel, velocity))
                if pitch_wheel != 0:
				midi.events.PitchWheelEvent(tick=int(ticks_to_this_tone),                                           pitch=pitch_wheel)
                song_events_absolute_ticks.append((abs_tick_note_beginning,
                                                   midi.events.NoteOnEvent(
                                                       tick=0,
                                                       velocity=velocity,
                                                       pitch=tone)))
                song_events_absolute_ticks.append((abs_tick_note_beginning + tick_len,
                                                   midi.events.NoteOffEvent(
                                                       tick=0,
                                                       velocity=0,
                                                       pitch=tone)))
    song_events_absolute_ticks.sort(key=lambda e: e[0])
    abs_tick_note_beginning = 0.0
    for abs_tick, event in song_events_absolute_ticks:
        rel_tick = abs_tick - abs_tick_note_beginning
        event.tick = int(round(rel_tick))
        cur_track.append(event)
        abs_tick_note_beginning = abs_tick

    cur_track.append(midi.EndOfTrackEvent(tick=int(output_ticks_per_quarter_note)))
    midi_pattern.append(cur_track)
    print 'Printing midi track.'
    print midi_pattern
    return midi_pattern


def save_midi_pattern(filename, midi_pattern):
    if filename is not None:
        midi.write_midifile(filename, midi_pattern)


def save_data(filename, song_data):
    """
    returns the midi_pattern.
    Can be used with filename == None. Then nothing is saved, but only returned.
    """
    midi_pattern = get_midi_pattern(song_data)
    save_midi_pattern(filename, midi_pattern)
    return midi_pattern


MUSIC_DIR = "./dataset/MIDIs"
file_list = []
song_data = []
files = os.listdir(MUSIC_DIR)
i = 0
for f in files:
    if (os.path.isfile(MUSIC_DIR + '/' + f)):
        i = i + 1
        file_list.append(f)
        song = read_one_file(MUSIC_DIR, f, False)
        print song
        song = np.array(song)
        print song
        for k in range(len(song) - 1):
            song[k, 0] = song[k + 1, 0] - song[k, 0]
        song[len(song) - 1, 0] = 0
        song_data.append(song)
        print(song.shape)
        print(song)
pkl.dump(song_data, open("./midi_out", "wb"))
```

